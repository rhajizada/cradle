# yaml-language-server: $schema=https://raw.githubusercontent.com/rhajizada/cradle/refs/heads/main/configuration.schema.json
version: 1
aliases:
  almalinux9:
    image:
      build:
        cwd: ./images/almalinux9
        dockerfile: Dockerfile
        args:
          USERNAME: ${USER}
          UID: ${UID}
    run:
      work_dir: /home/${USER}
      cmd: ["/bin/bash"]
      tty: true
      stdin_open: true
      attach: true
      network_mode: host
      volumes:
        - type: bind
          source: /home
          target: /home
  archlinux:
    image:
      build:
        cwd: ./images/archlinux
        dockerfile: Dockerfile
        args:
          USERNAME: ${USER}
          UID: ${UID}
    run:
      work_dir: /home/${USER}
      cmd: ["/bin/bash"]
      tty: true
      stdin_open: true
      attach: true
      network_mode: host
      volumes:
        - type: bind
          source: /home
          target: /home
  badgen:
    image:
      pull:
        ref: amio/badgen
    run:
      ports:
        - "3000:3000"
      attach: false
  code:
    image:
      pull:
        ref: lscr.io/linuxserver/code-server:latest
    run:
      ports:
        - "8443:8443"
      env:
        PUID: "1000"
        PGID: "1000"
        TZ: Etc/UTC
      volumes:
        - type: tmpfs
          target: /config
      attach: false
  debian12:
    image:
      build:
        cwd: ./images/debian12
        dockerfile: Dockerfile
        args:
          USERNAME: ${USER}
          UID: ${UID}
    run:
      work_dir: /home/${USER}
      cmd: ["/bin/bash"]
      tty: true
      stdin_open: true
      attach: true
      network_mode: host
      volumes:
        - type: bind
          source: /home
          target: /home
  echo:
    image:
      pull:
        ref: hashicorp/http-echo
    run:
      ports:
        - "5678:5678"
      cmd: ["-text=hello", "-listen=:5678"]
      attach: false
  fedora44:
    image:
      build:
        cwd: ./images/fedora44
        dockerfile: Dockerfile
        args:
          USERNAME: ${USER}
          UID: ${UID}
    run:
      work_dir: /home/${USER}
      cmd: ["/bin/bash"]
      tty: true
      stdin_open: true
      attach: true
      network_mode: host
      volumes:
        - type: bind
          source: /home
          target: /home
  opensuse16:
    image:
      build:
        cwd: ./images/opensuse16
        dockerfile: Dockerfile
        args:
          USERNAME: ${USER}
          UID: ${UID}
    run:
      work_dir: /home/${USER}
      cmd: ["/bin/bash"]
      tty: true
      stdin_open: true
      attach: true
      network_mode: host
      volumes:
        - type: bind
          source: /home
          target: /home
  nvidia-smi:
    image:
      pull:
        ref: ubuntu
    run:
      runtime: nvidia
      gpus:
        - count: all
      cmd: ["nvidia-smi"]
      tty: true
      stdin_open: true
      attach: true
  rocky9:
    image:
      build:
        cwd: ./images/rocky9
        dockerfile: Dockerfile
        args:
          USERNAME: ${USER}
          UID: ${UID}
    run:
      work_dir: /home/${USER}
      cmd: ["/bin/bash"]
      tty: true
      stdin_open: true
      attach: true
      network_mode: host
      volumes:
        - type: bind
          source: /home
          target: /home
  transmission:
    image:
      pull:
        ref: lscr.io/linuxserver/transmission:latest
    run:
      ports:
        - "9091:9091"
        - "51413:51413"
        - "51413:51413/udp"
      env:
        PUID: "1000"
        PGID: "1000"
        TZ: Etc/UTC
      volumes:
        - type: tmpfs
          target: /config
        - type: bind
          source: ${HOME}/Downloads
          target: /downloads
      attach: false
  ubuntu2404:
    image:
      build:
        cwd: ./images/ubuntu2404
        dockerfile: Dockerfile
        args:
          USERNAME: ${USER}
          UID: ${UID}
    run:
      work_dir: /home/${USER}
      cmd: ["/bin/bash"]
      tty: true
      stdin_open: true
      attach: true
      network_mode: host
      volumes:
        - type: bind
          source: /home
          target: /home
  vllm-qwen3:
    image:
      pull:
        policy: if_missing
        ref: vllm/vllm-openai:cu130-nightly
    run:
      ipc: host
      runtime: nvidia
      gpus:
        - count: all
      cmd:
        - "--max-model-len"
        - "40960"
        - "--gpu-memory-utilization"
        - "0.5"
        - "--max-num-seqs"
        - "1"
        - "Qwen/Qwen3-0.6B"
      volumes:
        - type: bind
          source: /dev/null
          target: /etc/ld.so.conf.d/cuda-compat.conf
        - type: bind
          source: /scr/huggingface
          target: /root/.cache/huggingface
